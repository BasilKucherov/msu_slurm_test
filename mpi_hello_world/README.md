# MPI  "Hallo, world" с помощью Slurm и Enroot

В этом репозитории представлен простой пример MPI "Hallo, world" для запуска на кластере HPC с помощью Slurm и Enroot. Запуск включает в себя:
- компиляцию внутри контейнера Enroot (`compile.slurm`)
- запуск программы на одном узле (`single_node.slurm`)
- запуск программы на нескольких узлах (`multi_node.slurm`)

## Предварительные требования
- Доступ к кластеру HPC с помощью **Slurm** и **Enroot**
- Контейнер, совместимый с Enroot, с поддержкой MPI (например, `nvcr.io/nvidia/pytorch:24.04-py3.sqsh`)
- Общая файловая система (`/scratch/$USER/`) для хранения файлов

## Структура репозитория
```
mpi_hello_world/
├── hello_world.c        # Простая программа MPI
├── Makefile             # Инструкции по компиляции
├── compile.slurm        # Скрипт Slurm для компиляции
├── single_node.slurm    # Скрипт Slurm для одноузлового выполнения
├── multi_node.slurm     # Скрипт Slurm для многоузлового выполнения
└── README.md            # Этот файл
```

---

## **1. Компиляция**
Чтобы скомпилировать программу MPI внутри контейнера Enroot, отправьте следующее задание:
```bash
sbatch compile.slurm
```
Это позволит:
- запустить задание с использованием контейнера
- скомпилировать `hello_world.c`
- создать исполняемый файл `hello_world` в `/scratch/$USER/mpi_hello_world/`

### **Проверка результатов компиляции**
Отслеживайте статус задания с помощью:
```bash
squeue -u $USER
```

Проверьте журналы после завершения:
```bash
cat /scratch/$USER/mpi_hello_world/compile.out
```

---

## **2. Запуск на одном узле**
После компиляции вы можете запустить программу на одном узле с помощью:
```bash
sbatch single_node.slurm
```
Этот скрипт:
- запрашивает 1 узел с несколькими процессами MPI
- Запускает скомпилированную программу `hello_world`

### **Проверка результатов выполнения**
```bash
cat /scratch/$USER/mpi_hello_world/single_node.out
```

---

## **3. Запуск на нескольких узлах**
Для запуска на нескольких узлах:
```bash
sbatch multi_node.slurm
```
Этот скрипт:
- запрашивает несколько узлов
- Использует `srun --mpi=pmix` для запуска процессов MPI на разных узлах

### **Проверка вывода результатов выполнения**
```bash
cat /scratch/$USER/mpi_hello_world/multi_node.out
```

---

## **Устранение неполадок**
- Если программа MPI не запускается, проверьте журналы `*.out` на наличие ошибок.
- Если двоичный файл отсутствует, повторно запустите `sbatch compile.slurm`.
- Убедитесь, что в среде Slurm включен MPI (может потребоваться `module load mpi`).
